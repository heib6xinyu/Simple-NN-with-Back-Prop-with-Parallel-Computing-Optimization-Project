[INFO   ] Using an SOFTMAX loss function.
[INFO   ] Creating a neural network with 2 hidden layers.
[INFO   ] Input layer 0 has 126 nodes.
[INFO   ] Hidden layer 1 has 10 nodes.
[INFO   ] Hidden layer 2 has 10 nodes.
[INFO   ] Output layer 3 has 2 nodes.
[INFO   ] Starting minibatch gradient descent!
[INFO   ] minibatch(20), mushroom, softmax, lr: 0.010000, mu:0.900000
[INFO   ]   0.699790 0.699790 41.334318
[INFO   ]   0.324342 0.324342 98.916790
[INFO   ]   0.324227 0.324227 98.916790
[INFO   ]   0.324184 0.324184 98.916790
[INFO   ]   0.324162 0.324162 98.916790
[INFO   ]   0.324149 0.324149 98.916790
[INFO   ]   0.324140 0.324140 98.916790
[INFO   ]   0.324133 0.324133 98.916790
[INFO   ]   0.324128 0.324128 98.916790
[INFO   ]   0.324125 0.324125 98.916790
[INFO   ]   0.324121 0.324121 98.916790
[INFO   ]   0.324119 0.324119 98.916790
[INFO   ]   0.324117 0.324117 98.916790
[INFO   ]   0.324115 0.324115 98.916790
[INFO   ]   0.324113 0.324113 98.916790
[INFO   ]   0.324112 0.324112 98.916790
[INFO   ]   0.324111 0.324111 98.916790
[INFO   ]   0.324110 0.324110 98.916790
[INFO   ]   0.324109 0.324109 98.916790
[INFO   ]   0.324108 0.324108 98.916790
[INFO   ]   0.324107 0.324107 98.916790
[INFO   ]   0.324107 0.324107 98.916790
[INFO   ]   0.324106 0.324106 98.916790
[INFO   ]   0.324106 0.324106 98.916790
[INFO   ]   0.324105 0.324105 98.916790
[INFO   ]   0.324105 0.324105 98.916790
[INFO   ]   0.324104 0.324104 98.916790
[INFO   ]   0.324104 0.324104 98.916790
[INFO   ]   0.324104 0.324104 98.916790
[INFO   ]   0.324103 0.324103 98.916790
[INFO   ]   0.324103 0.324103 98.916790
[INFO   ]   0.324103 0.324103 98.916790
[INFO   ]   0.324102 0.324102 98.916790
[INFO   ]   0.324102 0.324102 98.916790
[INFO   ]   0.324102 0.324102 98.916790
[INFO   ]   0.324102 0.324102 98.916790
[INFO   ]   0.324101 0.324101 98.916790
[INFO   ]   0.324101 0.324101 98.916790
[INFO   ]   0.324101 0.324101 98.916790
[INFO   ]   0.324101 0.324101 98.916790
[INFO   ]   0.324101 0.324101 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324100 0.324100 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324099 0.324099 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324098 0.324098 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
[INFO   ]   0.324097 0.324097 98.916790
